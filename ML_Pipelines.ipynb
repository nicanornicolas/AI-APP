{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMwj29zPMITrBMclzYWwkXg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicanornicolas/AI-APP/blob/main/ML_Pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Vertex AI SDK for Python and other required packages."
      ],
      "metadata": {
        "id": "CpXkAyJxpSDB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czBBAimfEeDI",
        "outputId": "0934517e-4d9e-41fd-b4bc-1141b41a3ec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/269.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.1/269.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m121.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m122.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for kfp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
            "spacy 3.8.7 requires thinc<8.4.0,>=8.3.4, but you have thinc 9.1.1 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet \\\n",
        "    google-cloud-aiplatform \\\n",
        "    google-cloud-storage \\\n",
        "    kfp \\\n",
        "    google-cloud-pipeline-components \\\n",
        "    ydf \\\n",
        "    opencv-python-headless \\\n",
        "    thinc \\\n",
        "    numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Authenticating the environment on Google Colab\n",
        "## Restarts the runtime.(Colab only)"
      ],
      "metadata": {
        "id": "1MkQvjPvrvtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "  import IPython\n",
        "\n",
        "  app = IPython.Application.instance()\n",
        "  app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "2OoJhMGsr4cS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Authenticate the notebook environment.\n",
        "\n",
        "## For Google Colab only."
      ],
      "metadata": {
        "id": "WL4ESqnNsp0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "\n",
        "  from google.colab import auth\n",
        "\n",
        "  auth.authenticate_user()"
      ],
      "metadata": {
        "id": "j1l20E0Ws1_Q"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the Google Cloud Project information."
      ],
      "metadata": {
        "id": "v0GV66lztp91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"valiant-imagery-464509-h0\"\n",
        "LOCATION = \"us-central1(iowa)\""
      ],
      "metadata": {
        "id": "NvqiACtCtw9l"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a Cloud Storage bucket.\n",
        "{Note: For any user-provided strings that need to be unique(like bucket names or model IDs), append \"-unique\" to the end so proper testing can occur}"
      ],
      "metadata": {
        "id": "pVTXGy_R-VZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUCKET_URI = f\"gs://demo_bucket101-valiant-imagery-464509-h0-unique\""
      ],
      "metadata": {
        "id": "vit4okCO-z1r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run this cell only if your bucket doesn't already exist to create a Cloud Storage bucket."
      ],
      "metadata": {
        "id": "Fx5um674_dQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! gsutil mb -1 {LOCATION} -p {PROJECT_ID} {BUCKET_URL}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bH5flcL_u5k",
        "outputId": "00fa60a1-f552-4156-a4eb-184b27d43b5b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CommandException: Incorrect option(s) specified. Usage:\n",
            "\n",
            "  gsutil mb [-b (on|off)] [-c <class>] [-k <key>] [-l <location>] [-p <project>]\n",
            "            [--autoclass] [--retention <time>] [--pap <setting>]\n",
            "            [--placement <region1>,<region2>]\n",
            "            [--rpo (ASYNC_TURBO|DEFAULT)] gs://<bucket_name>...\n",
            "\n",
            "For additional help run:\n",
            "  gsutil help mb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Service Account."
      ],
      "metadata": {
        "id": "-elqPlpQACKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SERVICE_ACCOUNT = \"my-service-account\""
      ],
      "metadata": {
        "id": "h9G0fL1FAGKo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run this cell if you don't know the service account, to get the service account using gcloud command."
      ],
      "metadata": {
        "id": "4N-iafsdAQZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if (\n",
        "    SERVICE_ACCOUNT == \"\"\n",
        "    or SERVICE_ACCOUNT is None\n",
        "    or SERVICE_ACCOUNT == \"my-service-account\"\n",
        "):\n",
        "    # Get your service account from gcloud\n",
        "    if not IS_COLAB:\n",
        "      shell_output = !gcloud auth list 2>/dev/null\n",
        "      SERVICE_ACCOUNT = shell_output[2].replace(\"*\",\"\").strip()\n",
        "\n",
        "    if IS_COLAB:\n",
        "      shell_output = !gcloud projects describe $PROJECT_ID\n",
        "      project_number = None\n",
        "      for line in shell_output:\n",
        "        if \"projectNumber\" in line:\n",
        "          project_number = line.split(\":\")[1].strip().replace(\"'\",\"\")\n",
        "          break\n",
        "      if project_number:\n",
        "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "\n",
        "    print(\"Service Account:\", SERVICE_ACCOUNT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysRet9j1Ag-A",
        "outputId": "a3a2e505-8676-4bbc-a72d-bdeea8aa1d65"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Service Account: 102963417104-compute@developer.gserviceaccount.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set service account access for Vertex AI Pipelines.\n",
        "## Run these commands to grant your service account access tp read and write pipeline artifacts in the bucket that was created in the previous step - this is required to only run once per service account."
      ],
      "metadata": {
        "id": "KnUdgXX_D94e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! gsutil iam ch serviceAccount: 102963417104-compute@developer.gserviceaccount.com:roles/storage.objectCreator $BUCKET_URI\n",
        "\n",
        "! gsutil iam ch serviceAccount: 102963417104-compute@developer.gserviceaccount.com:roles/storage.objectViewer $BUCKET_URI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsLDc5EvEhSo",
        "outputId": "18d0c726-569e-4345-8a7b-62257fbf9849"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CommandException: Must specify a role to grant.\n",
            "CommandException: Must specify a role to grant.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up variables\n",
        "\n",
        "## Import libraries and define constants"
      ],
      "metadata": {
        "id": "DGbeCXlLFgx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import NamedTuple\n",
        "\n",
        "import kfp\n",
        "from google.cloud import aiplatform\n",
        "from kfp import compiler, dsl\n",
        "from kfp.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, component)"
      ],
      "metadata": {
        "id": "GiVyQCoVFsCs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vertex AI Pipelines constants.\n",
        "\n",
        " Set up the following constants for Vertex AI Pipelines:"
      ],
      "metadata": {
        "id": "gjxSsGOwH1cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PIPELINE_ROOT = \"{}/pipeline_root/shakespeare\".format(BUCKET_URI)"
      ],
      "metadata": {
        "id": "eBTbyPSFIJab"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Vertex AI SDK for Python"
      ],
      "metadata": {
        "id": "ZiCtYqO3I6pG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
      ],
      "metadata": {
        "id": "15UTPo0NJGDf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Python function-based pipeline components.\n",
        "Define function-based components that consume parameters and produce (typed) Artifacts and parameters.\n",
        "Functions can produce Artifacts in 3 ways:\n",
        "  - Accept an output local path using OutputPath\n",
        "  - Accept an OutputArtifact which gives the function a handle to the output artifact's metadata.\n",
        "  - Return an Artifact(or Dataset, Model, Metrics, etc) in a NamedTuple.\n",
        "\n",
        "  // Options for producing Artifacts are demonstrated.\n",
        "\n",
        "  ## Define preprocess component.\n",
        "\n",
        "  The first component definition, preprocess, shows a component that outputs two Dataset Artifacts, as well as an output parameter. (In this example, the datasets do no reflect real data).\n",
        "\n",
        "  For the parameter output, you would typically use the approach shown here, using the OutputPath type, for the \"larger\" data. For \"small data\", like a short string, it might be more convenient to use the NamedTuple function output as shown in the second component instead.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JkcsmgEiJZJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@component(base_image=\"python:3.9\")\n",
        "def preprocess(\n",
        "    # An input parameter of type string.\n",
        "    message: str,\n",
        "    # Use Output to get a metadata-rich handle to the output artifact\n",
        "    # of type Dataset.\n",
        "    output_dataset_one: Output[Dataset],\n",
        "    # A locally accessible filepath for another output artifact of type\n",
        "    # Dataset.\n",
        "    output_dataset_two_path: OutputPath(\"Dataset\"),\n",
        "    # A locally accessible filepath for an output parameter of type string.\n",
        "    output_parameter_path: OutputPath(str),\n",
        "):\n",
        "  \"\"\"'Mock' preprocessing step.\n",
        "  Writes out the passed in message to the output \"Dataset's and the output message.\n",
        "  \"\"\"\n",
        "  output_dataset_one.metadata[\"hello\"] = \"there\"\n",
        "  # Use OutputArtifact.path to access a local file path for writing.\n",
        "  # One can also use OutputArtifact.uri to access the actual URI file path.\n",
        "  with open(output_dataset_one.path, \"w\") as f:\n",
        "    f.write(message)\n",
        "\n",
        "  # OutputPath is used to just pass the local file path of the output artifact\n",
        "  # to the function.\n",
        "  with open(output_dataset_two_path, \"w\") as f:\n",
        "    f.write(message)\n",
        "\n",
        "  # Output parameters are written to the output \"String\"\n",
        "  with open(output_parameter_path, \"w\") as f:\n",
        "    f.write(message)"
      ],
      "metadata": {
        "id": "IcZPIJ8vL0iE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Train component.\n",
        "\n",
        "The second component definition, train, defines as input both an InputPath of type Dataset, and an InputArtifact of type Dataset(as well as other parameter inputs). It uses the NamedTuple format for function output. As shown, these outputs can be Artifacts as well as parameters.\n",
        "\n",
        "Additionally, this component writes some metrics. Metadata to the model output Artifact. This information is displayed in the Cloud Console user interface when the pipeline runs.\n",
        "\n"
      ],
      "metadata": {
        "id": "L-5kngVIO3tL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@component(\n",
        "    base_image=\"python:3.10\", # Use a different base image.\n",
        ")\n",
        "def train(\n",
        "    # An input parameter of type string.\n",
        "    message: str,\n",
        "    # Use InputPath to get a locally accessible path for the input artifact\n",
        "    # of type Dataset.\n",
        "    dataset_one_path: InputPath(\"Dataset\"),\n",
        "    # Use InputArtifact to get a metadata-rich handle to the input artifact\n",
        "    # of type 'Dataset'.\n",
        "    dataset_two: Input[Dataset],\n",
        "    # Output artifact of type Model.\n",
        "    imported_dataset: Input[Dataset],\n",
        "    model: Output[Model],\n",
        "    # An input parameter of type int with a default value.\n",
        "    num_steps: int = 3,\n",
        "    # Use NamedTuple to return either artifacts or parameters.\n",
        "    # When returning artifacts like this, return the contents of\n",
        "    # the artifact. The assumption here is that this return value\n",
        "    # fits in memory.\n",
        "    ) -> NamedTuple(\n",
        "    \"Outputs\",\n",
        "    [\n",
        "        (\"output_message\", str),  # Return parameter.\n",
        "        (\"generic_artifact\", Artifact),  # Return generic Artifact.\n",
        "    ],\n",
        "):\n",
        "    \"\"\"'Mock' Training step.\n",
        "    Combines the contents of dataset_one and dataset_two into the output Model.\n",
        "    Constructs a new output_message consisting of message repeated num_steps times.\n",
        "    \"\"\"\n",
        "\n",
        "    # Directly access the passed-in GCS URI as a local file (uses GCSFuse).\n",
        "    with open(dataset_one_path, \"r\") as input_file:\n",
        "      dataset_one_contents = input_file.read()\n",
        "\n",
        "    # dataset_two is an Artifact handle. Use dataset_two.path to get a\n",
        "    # local file path (uses GCSFuse).\n",
        "    # Alternatively, use dataset_two.uri to access the GCS URI directly.\n",
        "    with open(dataset_two.path, \"r\") as input_file:\n",
        "      dataset_two_contents = input_file.read()\n",
        "\n",
        "    with open(model.path, \"w\") as f:\n",
        "      f.write(\"My Model\")\n",
        "\n",
        "    with open(imported_dataset.path, \"r\") as f:\n",
        "      data = f.read()\n",
        "    print(\"Imported Dataset:\", data)\n",
        "\n",
        "    # Use model.get() to get a Model artifact, which has a .metadata dictionary\n",
        "    # to store arbitrary metadata for the output artifact. This metadata is\n",
        "    # recorded in Managed Metadata and can be queried later. It also shows up\n",
        "    # in the Google Cloud Console.\n",
        "    model.metadata[\"accuracy\"] = 0.9\n",
        "    model.metadata[\"framework\"] = \"TensorFlow\"\n",
        "    model.metadata[\"time_to_train_in_seconds\"] = 264\n",
        "\n",
        "    artifact_contents = \"{}\\n{}\".format(dataset_one_contents, dataset_two_contents)\n",
        "    output_message = \" \".join([message for _ in range(num_steps)])\n",
        "    return (output_message, Artifact(uri=model.uri, metadata=model.metadata))"
      ],
      "metadata": {
        "id": "eW7P47xMP7NN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define read_artifact_input component.\n",
        "\n",
        "Finally, you define a small component that takes as input the generic_artifact returned by the train component function, and reads and prints the Artifact's contents."
      ],
      "metadata": {
        "id": "d6Nd_usdnacV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@component(base_image=\"python:3.9\")\n",
        "def read_artifact_input(\n",
        "    generic_artifact: Input[Artifact],\n",
        "):\n",
        "    with open(generic_artifact.path, \"r\") as input_file:\n",
        "      generic_contents = input_file.read()\n",
        "    print(\"Generic Artifact Contents:\", generic_contents)\n"
      ],
      "metadata": {
        "id": "Nqcp9y-Qn9jC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a pipeline that uses the components and the importer.\n",
        "\n",
        "The next step is to define a pipeline that uses the components that have been build, and also shows the use of kfp.dsl.importer.\n",
        "\n",
        "In this example, the importer is used for creation, in our case, a Dataset artifact from an existing URI.\n",
        "\n",
        "Note that the train_task step takes as inputs 3 of the outputs of the preprocess_task step, as well as the output of the importer step. In the \"train\" inputs, we refer to the preprocess output_parameter, which gives us the output string directly.\n",
        "\n",
        "The read_task step takes as input the train_task generic artifact output."
      ],
      "metadata": {
        "id": "c8ZEmU-8p3bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dsl.pipeline(\n",
        "    # Default pipeline root. You can override it when submitting the pipeline.\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    # A name for the pipeline. Use to determine the pipeline Context.\n",
        "    name=\"metadata-pipeline-v2\",\n",
        ")\n",
        "def pipeline(message: str):\n",
        "    importer = kfp.dsl.importer(\n",
        "        artifact_uri=\"gs://ml-pipeline-playground/shakespeare1.txt\",\n",
        "        artifact_class=Dataset,\n",
        "        reimport=False,\n",
        "    )\n",
        "    preprocess_task = preprocess(message=message)\n",
        "    train_task = train(\n",
        "        dataset_one_path=preprocess_task.outputs[\"output_dataset_one\"],\n",
        "        dataset_two=preprocess_task.outputs[\"output_dataset_two_path\"],\n",
        "        imported_dataset=importer.output,\n",
        "        message=preprocess_task.outputs[\"output_parameter_path\"],\n",
        "        num_steps=10,\n",
        "    )\n",
        "    read_task = read_artifact_input(\n",
        "        generic_artifact=train_task.outputs[\"generic_artifact\"])"
      ],
      "metadata": {
        "id": "ujFaq8P3rlL4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile the pipeline"
      ],
      "metadata": {
        "id": "InuNhsUht0fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compiler.Compiler().compile(\n",
        "    pipeline_func=pipeline, package_path=\"lightweight_pipeline.yaml\"\n",
        ")"
      ],
      "metadata": {
        "id": "AFkpxKWBt8O5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the pipeline"
      ],
      "metadata": {
        "id": "C1TbjFBjuR0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DISPLAY_NAME = \"shakespeare\"\n",
        "\n",
        "job = aiplatform.PipelineJob(\n",
        "    display_name=DISPLAY_NAME,\n",
        "    template_path=\"lightweight_pipeline.yaml\",\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    parameter_values={\"message\": \"Hello World\"},\n",
        "    enable_caching=False,\n",
        ")\n",
        "\n",
        "job.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZYQbQseubsK",
        "outputId": "8a8058d9-dfd4-4549-b559-c221f26714be"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
          ]
        }
      ]
    }
  ]
}